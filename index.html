
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Stephen Hope">
    <meta name="description" content="Complete Guide to Configure VS Code for Python and AI Agent Development">
    <meta name="keywords" content="VS Code, Python, AI, LangChain, FastAPI, RAG, Development">
    <meta name="last-updated" content="2025-08-05">

    <title>Complete Guide to Configure VS Code for Python and AI Agent Development</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        :root {
            --primary: #2c3e50;
            --secondary: #3498db;
            --success: #27ae60;
            --warning: #f39c12;
            --danger: #e74c3c;
            --light: #ecf0f1;
            --dark: #2c3e50;
            --gray: #95a5a6;
            --light-gray: #f8f9fa;
            --border-radius: 8px;
            --box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            --transition: all 0.3s ease;
        }
       
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
       
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f7fa;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
       
        h1, h2, h3, h4 {
            color: var(--primary);
            margin-bottom: 1rem;
        }
       
        h1 {
            font-size: 2.5rem;
            padding-bottom: 15px;
            border-bottom: 3px solid var(--primary);
            margin-top: 1.5rem;
            text-align: center;
        }
       
        h2 {
            font-size: 2rem;
            margin-top: 2.5rem;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--secondary);
        }
       
        h3 {
            font-size: 1.5rem;
            margin-top: 2rem;
            color: var(--secondary);
        }
       
        h4 {
            font-size: 1.2rem;
            margin-top: 1.5rem;
            color: #555;
        }
       
        p {
            margin-bottom: 1rem;
        }
       
        a {
            color: var(--secondary);
            text-decoration: none;
            transition: var(--transition);
        }
       
        a:hover {
            color: var(--primary);
            text-decoration: underline;
        }
       
        code {
            background: var(--light-gray);
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Fira Code', 'Consolas', monospace;
            font-size: 0.95em;
        }
       
        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 1.2rem;
            border-radius: var(--border-radius);
            overflow-x: auto;
            margin: 1.2rem 0;
            line-height: 1.5;
            font-family: 'Fira Code', 'Consolas', monospace;
            font-size: 0.95em;
        }
       
        pre code {
            background: none;
            padding: 0;
            border-radius: 0;
        }
       
        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }
       
        li {
            margin-bottom: 0.6rem;
        }

        .card {
            background: white;
            border-radius: var(--border-radius);
            box-shadow: var(--box-shadow);
            padding: 2rem;
            margin-bottom: 2rem;
        }
       
        .note, .warning, .critical, .quick-start {
            padding: 1.5rem;
            border-radius: var(--border-radius);
            margin: 1.5rem 0;
            border-left: 5px solid;
        }
       
        .note {
            background-color: #e7f3fe;
            border-color: #2196F3;
        }
       
        .warning {
            background-color: #fff3cd;
            border-color: #ffc107;
        }
       
        .critical {
            background-color: #f8d7da;
            border-color: #dc3545;
        }
       
        .quick-start {
            background-color: #d4edda;
            border-color: #28a745;
        }
       
        .section {
            margin-bottom: 2.5rem;
        }
       
        .version-matrix, .framework-comparison {
            background: var(--light-gray);
            border: 1px solid #ddd;
            border-radius: var(--border-radius);
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
        }
       
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            min-width: 600px;
        }
       
        th, td {
            border: 1px solid #ddd;
            padding: 1rem;
            text-align: left;
        }
       
        th {
            background-color: #f2f2f2;
            font-weight: 600;
        }
       
        .code-container {
            position: relative;
            margin: 1.5rem 0;
            background: #2d2d2d;
            border-radius: var(--border-radius);
            overflow: hidden;
        }
       
        .code-block {
            padding: 1.5rem;
            font-family: 'Fira Code', 'Consolas', monospace;
            line-height: 1.5;
            overflow-x: auto;
            color: #f8f8f2;
        }
       
        .copy-button {
            position: absolute;
            top: 0.8rem;
            right: 0.8rem;
            padding: 0.5rem 1rem;
            background: var(--primary);
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            opacity: 0.8;
            transition: var(--transition);
            font-size: 0.9rem;
        }
       
        .copy-button:hover {
            opacity: 1;
            background: var(--secondary);
        }
       
        .copy-button.copied {
            background: var(--success);
        }
       
        .part-nav {
            display: flex;
            justify-content: center;
            margin: 2.5rem 0;
            gap: 15px;
            flex-wrap: wrap;
        }
       
        .part-link {
            padding: 15px 25px;
            background: var(--primary);
            color: white;
            border-radius: var(--border-radius);
            font-weight: 600;
            transition: var(--transition);
            text-align: center;
            flex: 1;
            max-width: 300px;
            min-width: 250px;
        }
       
        .part-link:hover {
            background: var(--secondary);
            text-decoration: none;
            transform: translateY(-3px);
        }
       
        .audience-box {
            background: #e8f4fc;
            border-left: 4px solid var(--secondary);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: var(--border-radius);
        }
       
        .next-steps {
            background: #e8f5e9;
            border-left: 4px solid var(--success);
            padding: 2rem;
            margin: 2.5rem 0;
            border-radius: var(--border-radius);
            text-align: center;
        }
       
        .cta-button {
            display: inline-block;
            padding: 15px 30px;
            background: var(--success);
            color: white;
            border-radius: var(--border-radius);
            font-weight: 600;
            margin: 15px 0;
            transition: var(--transition);
        }
       
        .cta-button:hover {
            background: #1e8449;
            text-decoration: none;
            transform: translateY(-2px);
        }
       
        .roadmap {
            list-style-type: none;
            padding-left: 0;
            margin: 1.5rem 0;
        }
       
        .roadmap li {
            position: relative;
            padding-left: 35px;
            margin-bottom: 1.2rem;
        }
       
        .roadmap li:before {
            content: counter(step);
            counter-increment: step;
            position: absolute;
            left: 0;
            top: 0;
            width: 28px;
            height: 28px;
            background: var(--secondary);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 1rem;
        }
       
        .resource-card {
            background: white;
            border-radius: var(--border-radius);
            box-shadow: var(--box-shadow);
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }
       
        .resource-card h3 {
            margin-top: 0;
            display: flex;
            align-items: center;
            gap: 12px;
        }
       
        .icon {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 32px;
            height: 32px;
            background: var(--secondary);
            color: white;
            border-radius: 50%;
            margin-right: 10px;
        }
       
        .progress-bar {
            height: 8px;
            background: #e0e0e0;
            border-radius: 4px;
            margin: 1.5rem 0;
            overflow: hidden;
        }
       
        .progress {
            height: 100%;
            background: var(--success);
            width: 33%;
        }
       
        .timeline {
            display: flex;
            justify-content: space-between;
            margin: 0.8rem 0 2rem;
            font-size: 0.95rem;
            color: var(--gray);
        }
       
        .part-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2rem;
            padding-bottom: 15px;
            border-bottom: 2px solid var(--secondary);
        }
       
        .time-estimate {
            background: var(--secondary);
            color: white;
            padding: 6px 12px;
            border-radius: 20px;
            font-size: 1rem;
            font-weight: 600;
        }
       
        .tooltip {
            position: relative;
            display: inline-block;
            cursor: pointer;
        }
       
        .tooltip .tooltip-text {
            visibility: hidden;
            width: 220px;
            background-color: var(--dark);
            color: white;
            text-align: center;
            border-radius: 6px;
            padding: 10px;
            position: absolute;
            z-index: 1;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            opacity: 0;
            transition: opacity 0.3s;
            font-size: 0.95rem;
        }
       
        .tooltip:hover .tooltip-text {
            visibility: visible;
            opacity: 1;
        }

        @media (prefers-color-scheme: dark) {
            body {
                background-color: #1e1e1e;
                color: #e0e0e0;
            }

            .card,
            .resource-card,
            .note,
            .warning,
            .critical,
            .quick-start,
            .audience-box,
            .version-matrix,
            .framework-comparison {
                background-color: #2b2b2b;
                color: #ffffff;
            }

            h1, h2, h3, h4 {
                color: #64b5f6;
            }

            a {
                color: #90caf9;
            }

            a:hover {
                color: #bbdefb;
            }

            code,
            pre {
                background-color: #1f1f1f;
                color: #c5c8c6;
            }

            .copy-button {
                background-color: #424242;
                color: #fff;
            }

            .copy-button:hover {
                background-color: #64b5f6;
            }

            .timeline,
            .tooltip .tooltip-text {
                color: #ccc;
                background-color: #424242;
            }

            .table th {
                background-color: #333;
            }
        }

        /* Sidebar and main content layout */
        .container {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
        }
        
        .sidebar {
            position: sticky;
            top: 20px;
            flex: 0 0 260px;
            max-width: 260px;
            background: var(--light);
            padding: 1.5rem;
            border-radius: var(--border-radius);
            box-shadow: var(--box-shadow);
            overflow-y: auto;
            max-height: calc(100vh - 40px);
        }
        
        .sidebar h2 {
            font-size: 1.5rem;
            margin-top: 0;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--secondary);
        }
        
        .sidebar ul {
            list-style: none;
            padding-left: 0;
        }
        
        .sidebar li {
            margin-bottom: 0.5rem;
        }
        
        .sidebar a {
            display: block;
            padding: 8px 12px;
            border-radius: 4px;
            color: var(--primary);
            transition: var(--transition);
        }
        
        .sidebar a:hover {
            background-color: rgba(52, 152, 219, 0.1);
            text-decoration: none;
        }
        
        .sidebar ul ul {
            padding-left: 1rem;
            margin-top: 0.5rem;
        }
        
        .sidebar ul ul a {
            padding: 6px 12px;
            font-size: 0.95rem;
        }
        
        .content {
            flex: 1;
            min-width: 0;
        }

        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            
            h1 {
                font-size: 2rem;
            }
            
            h2 {
                font-size: 1.7rem;
            }
            
            .card {
                padding: 1.5rem;
            }
            
            .part-link {
                min-width: 100%;
            }
            
            .container {
                flex-direction: column;
            }
            
            .sidebar {
                position: static;
                width: 100%;
                max-width: 100%;
                max-height: none;
            }
            
            table {
                font-size: 0.9rem;
            }
            
            th, td {
                padding: 0.8rem;
            }
        }
    </style>
</head>
<body>
    <a href="#main-content" class="skip-link" aria-label="Skip to main content">Skip to main content</a>

    <header>
        <h1>Complete Guide to Configure VS Code for Python and AI Agent Development</h1>
        <p><strong>Author:</strong> Stephen Hope | <strong>Version:</strong> 3.0 - August 2025</p>
        <p>This comprehensive guide helps you set up Visual Studio Code for building AI agents using Python and frameworks like LangChain, CrewAI, AutoGen, and LlamaIndex.</p>
    </header>

    <div class="critical">
        <h2><i class="fas fa-exclamation-circle"></i> CRITICAL UPDATES IN VERSION 3.0</h2>
        <p><strong>This version includes major fixes for critical compatibility issues:</strong></p>
        <ul>
            <li><strong>PyTorch updated to 2.7.0+</strong> with CUDA 12.1 support</li>
            <li><strong>LangChain-OpenAI updated to 0.3.x</strong> with new import syntax</li>
            <li><strong>All package versions refreshed</strong> for August 2025 compatibility</li>
            <li><strong>Python 3.13 compatibility verified</strong> with AI package support</li>
            <li><strong>New dependency conflict resolution strategies</strong></li>
        </ul>
        <p><strong>LangChain 0.3.x Critical Changes:</strong></p>
        <ul>
            <li>New import paths: <code>from langchain_openai import ChatOpenAI</code> instead of <code>from langchain.chat_models</code></li>
            <li>Message handling requires <code>HumanMessage</code>/<code>AIMessage</code> instead of strings</li>
            <li>Agent executor syntax changed to <code>agent.invoke()</code></li>
        </ul>
    </div>

    <div class="quick-start">
        <h2><i class="fas fa-bolt"></i> Quick Start (30 minutes)</h2>
        <p><strong>For users who want to get up and running quickly:</strong></p>
        <ol>
            <li><strong>Install basics</strong>: Download VS Code and Python 3.12</li>
            <li><strong>Create project</strong>: Make a folder, open in VS Code</li>
            <li><strong>Set up environment</strong>: Create virtual environment</li>
            <li><strong>Install essentials</strong>: <code>pip install openai python-dotenv langchain langchain-openai</code></li>
            <li><strong>Add API key</strong>: Create <code>.env</code> file</li>
            <li><strong>Test setup</strong>: Run a simple agent</li>
        </ol>
        <p><em>Skip to Part 2 after completing these steps, then return for advanced features later.</em></p>
    </div>

    <div class="warning">
        <p><strong>Cost Considerations:</strong> Services like OpenAI API and GitHub Copilot may incur costs. Free alternatives like Ollama are provided.</p>
        <p><strong>Package Evolution Warning:</strong> AI packages evolve rapidly. Check for newer versions if you're reading this after August 2025.</p>
        <div class="critical" style="margin-top: 15px;">
            <h4>‚ö†Ô∏è LangChain-OpenAI Conflict Resolution</h4>
            <p>If you encounter <code>AttributeError: module 'openai' has no attribute 'ChatCompletion'</code>:</p>
            <pre><code># Fix: Ensure compatible versions
pip uninstall -y langchain-openai openai
pip install openai==1.54.3 langchain-openai==0.3.28</code></pre>
        </div>
    </div>

    <div class="part-nav">
        <a href="#part1" class="part-link"><i class="fas fa-cogs"></i> Part 1: Basic Setup</a>
        <a href="#part2" class="part-link"><i class="fas fa-robot"></i> Part 2: AI Development</a>
        <a href="#part3" class="part-link"><i class="fas fa-server"></i> Part 3: Production</a>
    </div>

    <div class="container">
        <aside class="sidebar">
            <nav aria-label="Guide Navigation">
                <h2><i class="fas fa-bookmark"></i> Guide Contents</h2>
                <ul>
                    <li><a href="#part1"><strong>Part 1: Basic Setup</strong></a>
                        <ul>
                            <li><a href="#section1">VS Code & Prerequisites</a></li>
                            <li><a href="#section2">Python in VS Code</a></li>
                            <li><a href="#section3">Virtual Environment</a></li>
                            <li><a href="#section4">AI Extensions</a></li>
                            <li><a href="#section5">AI Packages</a></li>
                            <li><a href="#section6">GPU Setup</a></li>
                        </ul>
                    </li>
                    <li><a href="#part2"><strong>Part 2: AI Development</strong></a>
                        <ul>
                            <li><a href="#section7">Project Structure</a></li>
                            <li><a href="#section8">Debugging</a></li>
                            <li><a href="#section9">API Keys</a></li>
                            <li><a href="#section10">Verification</a></li>
                            <li><a href="#section11">First AI Agent</a></li>
                            <li><a href="#section15">Vector Databases</a></li>
                        </ul>
                    </li>
                    <li><a href="#part3"><strong>Part 3: Production</strong></a>
                        <ul>
                            <li><a href="#section12">Docker</a></li>
                            <li><a href="#section13">Experiment Tracking</a></li>
                            <li><a href="#section14">FastAPI Serving</a></li>
                            <li><a href="#section16">Code Quality</a></li>
                            <li><a href="#section20">Mini-Project</a></li>
                        </ul>
                    </li>
                    <li><a href="#shared-resources"><strong>Shared Resources</strong></a>
                        <ul>
                            <li><a href="#section18">Troubleshooting</a></li>
                            <li><a href="#section19">Changelog</a></li>
                            <li><a href="#quick-reference">Quick Reference</a></li>
                        </ul>
                    </li>
                </ul>
            </nav>
            <div class="resource-card">
                <h3><i class="fas fa-download"></i> Download Guide</h3>
                <p>Save this guide for offline reference:</p>
                <button id="download-pdf" class="cta-button" style="width:100%;">
                    <i class="fas fa-file-pdf"></i> Download PDF
                </button>
            </div>
            <div class="resource-card">
                <h3><i class="fas fa-history"></i> Last Updated</h3>
                <p>August 5, 2025</p>
                <p>Version 3.0</p>
            </div>
        </aside>

        <main class="content" id="main-content">
            <!-- Part 1: Basic Setup -->
            <section id="part1" class="card">
                <div class="part-header">
                    <h2><i class="fas fa-cogs"></i> Part 1: Basic Setup</h2>
                    <span class="time-estimate">60-90 min</span>
                </div>
               
                <div class="progress-bar">
                    <div class="progress" style="width: 33%"></div>
                </div>
                <div class="timeline">
                    <span>Getting Started</span>
                    <span>Development Ready</span>
                    <span>Production</span>
                </div>
               
                <div class="audience-box">
                    <h3><i class="fas fa-user"></i> Start Here If:</h3>
                    <ul>
                        <li>You're setting up VS Code for the first time</li>
                        <li>You're new to Python virtual environments</li>
                        <li>You need basic AI development configuration</li>
                    </ul>
                </div>
               
                <p>This section covers everything needed to get your development environment ready for AI work. You'll install VS Code, set up Python, configure virtual environments, and install essential tools.</p>
               
                <article class="section" id="section1">
                    <h3>1. Install VS Code and Prerequisites (Beginner)</h3>
                    <h4>1.1 Install VS Code</h4>
                    <ul>
                        <li><strong>Download</strong>: Get the stable version from <a href="https://code.visualstudio.com">code.visualstudio.com</a>.</li>
                        <li><strong>Install</strong>: Follow prompts for your OS (Windows, macOS, or Linux).</li>
                        <li><strong>Verify</strong>: Open VS Code and confirm it launches.</li>
                    </ul>

                    <h4>1.2 Install Python</h4>
                    <div class="version-matrix" aria-label="Version compatibility information">
                        <h4>Python Version Compatibility Matrix (August 2025)</h4>
                        <table>
                            <tr>
                                <th>Python Version</th>
                                <th>AI Library Support</th>
                                <th>Recommendation</th>
                                <th>Use Case</th>
                            </tr>
                            <tr>
                                <td>3.11.9</td>
                                <td>‚úÖ Universal support</td>
                                <td>üü¢ Stable choice</td>
                                <td>Production, maximum compatibility</td>
                            </tr>
                            <tr>
                                <td>3.12.7</td>
                                <td>‚úÖ Excellent support</td>
                                <td>üü¢ Recommended</td>
                                <td>New projects, modern features</td>
                            </tr>
                            <tr>
                                <td>3.13.5</td>
                                <td>‚ö†Ô∏è Limited AI support</td>
                                <td>üü° Use only for pure Python projects</td>
                                <td>PyTorch support incomplete, TensorFlow not supported</td>
                            </tr>
                        </table>
                    </div>
                    <ul>
                        <li><strong>Recommended Version</strong>: Python 3.12.7 from <a href="https://www.python.org/downloads/">python.org</a></li>
                        <li><strong>Installation Setup</strong>:
                            <ul>
                                <li><strong>Windows</strong>: Check "Add Python to PATH" during installation. After installation, open a *new* terminal window to verify.</li>
                                <li><strong>macOS</strong>: Use Homebrew: <code>brew install python@3.12</code>. For Apple Silicon (M1/M2/M3), ensure Rosetta 2 is installed (<code>softwareupdate --install-rosetta</code>) for compatibility with some packages.</li>
                                <li><strong>Linux</strong>: Use system package manager or <code>pyenv</code> for version management (e.g., <code>pyenv install 3.12.7</code>).</li>
                                <li><strong>Cross-Platform Note</strong>: For Windows users, consider using Windows Subsystem for Linux (WSL) for a more consistent, Linux-aligned development experience, which simplifies Docker usage and package management.</li>
                            </ul>
                        </li>
                        <li><strong>Verify Installation</strong>:
                            <pre><code># Check Python version
python --version
# or on some systems
python3 --version
# Should show: Python 3.12.7</code></pre>
                        </li>
                    </ul>

                    <h4>1.3 Install Git</h4>
                    <ul>
                        <li><strong>Download</strong>: Install Git from <a href="https://git-scm.com">git-scm.com</a>.</li>
                        <li><strong>Configure</strong>: Set up your name and email:
                            <pre><code>git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"</code></pre>
                        </li>
                        <li><strong>Verify</strong>: <code>git --version</code></li>
                    </ul>
                </article>
               
                <article class="section" id="section2">
                    <h3>2. Set Up Python in VS Code (Beginner)</h3>
                    <h4>2.1 Install the Python Extension</h4>
                    <ul>
                        <li>Open Extensions view (Ctrl+Shift+X / Cmd+Shift+X).</li>
                        <li>Search for <strong>Python</strong> (by Microsoft, ID: <code>ms-python.python</code>) and install.</li>
                        <li><strong>Features</strong>: Code completion (via Pylance), linting, debugging, Jupyter support.</li>
                        <li><strong>Note</strong>: Restart VS Code after installation to ensure all features load correctly.</li>
                    </ul>

                    <h4>2.2 Select Python Interpreter</h4>
                    <ul>
                        <li>Open Command Palette (Ctrl+Shift+P / Cmd+Shift+P).</li>
                        <li>Type <code>Python: Select Interpreter</code>.</li>
                        <li>Choose your Python installation (e.g., Python 3.12.7).</li>
                        <li>Verify in VS Code status bar (bottom-left shows Python version).</li>
                    </ul>
                </article>
               
                <article class="section" id="section3">
                    <h3>3. Virtual Environment Setup (Beginner)</h3>
                    <p><strong>Critical</strong>: Always use virtual environments to avoid dependency conflicts. We will use the standard name <code>.venv</code> for our environment.</p>

                    <h4>3.1 Create a Virtual Environment</h4>
                    <p>Choose one method based on your preference:</p>
                    <h4>Method 1: VS Code Integrated Creation (Easiest)</h4>
                    <ol>
                        <li>Open your project folder in VS Code.</li>
                        <li>Open the Command Palette (Ctrl+Shift+P / Cmd+Shift+P).</li>
                        <li>Type <code>Python: Create Environment</code> and select it.</li>
                        <li>Choose <code>Venv</code>.</li>
                        <li>Select the Python interpreter you installed (e.g., Python 3.12.7).</li>
                        <li>VS Code will create the environment in a <code>.venv</code> folder and automatically configure your workspace to use it.</li>
                    </ol>
                    <h4>Method 2: Manual Terminal Creation</h4>
                    <pre><code># Navigate to your project directory
cd your-ai-project

# Create virtual environment named ".venv"
python -m venv .venv

# --- Activate the environment ---
# Windows (Command Prompt):
.venv\Scripts\activate.bat

# Windows (PowerShell):
# Note: You may need to run 'Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process' first. This is a one-time setup step.
.venv\Scripts\Activate.ps1

# macOS/Linux (bash/zsh):
source .venv/bin/activate

# Verify activation (your terminal prompt should now be prefixed with (.venv))
which python</code></pre>

                    <h4>3.2 Configure VS Code to Use the Virtual Environment</h4>
                    <ul>
                        <li>If you used Method 1, VS Code does this automatically.</li>
                        <li>If you created it manually, open your project folder, press Ctrl+Shift+P / Cmd+Shift+P, type <code>Python: Select Interpreter</code>, and choose the interpreter from your virtual environment (e.g., <code>./.venv/bin/python</code>).</li>
                    </ul>
                </article>
               
                <article class="section" id="section4">
                    <h3>4. AI-Specific Extensions & Tools (Beginner)</h3>
                    <p>Install these from the VS Code marketplace:</p>
                    <ul>
                        <li><strong>Jupyter</strong> (<code>ms-toolsai.jupyter</code>): For notebook development and prototyping.</li>
                        <li><strong>GitHub Copilot</strong> (<code>GitHub.copilot</code>): AI-powered code completion. Requires a subscription.</li>
                        <li><strong>Python Docstring Generator</strong> (<code>njpwerner.autodocstring</code>): Auto-generate documentation.</li>
                        <li><strong>Black Formatter</strong> (<code>ms-python.black-formatter</code>): Code formatting.</li>
                        <li><strong>isort</strong> (<code>ms-python.isort</code>): Import sorting.</li>
                        <li><strong>Pylint</strong> (<code>ms-python.pylint</code>): Advanced linting.</li>
                        <li><strong>Error Lens</strong> (<code>usernamehw.errorlens</code>): Highlights errors/warnings in-line.</li>
                    </ul>
                    <p class="note"><strong>Note</strong>: Ensure <code>ipykernel</code> is installed in your virtual environment for Jupyter support (<code>pip install ipykernel</code>).</p>
                </article>
               
                <article class="section" id="section5">
                    <h3>5. Install Essential AI Packages (Intermediate)</h3>
                    <p>Ensure your virtual environment is activated. Choose one of the two paths below for managing your project's dependencies.</p>
                    
                    <div class="critical">
                        <h4>üö® UPDATED PACKAGE VERSIONS (August 2025)</h4>
                        <p>Critical fixes applied to all package versions for compatibility:</p>
                    </div>
                    
                    <h4>Path A: Simple Method (pip freeze)</h4>
                    <p>This method is straightforward. You install packages manually and then save a list of all dependencies.</p>
                    <ol>
                        <li><strong>Install Packages:</strong> Run the following commands to install the core libraries.
                            <pre><code># Update pip first
pip install --upgrade pip

# Core AI/ML and Data Science (Updated August 2025)
pip install numpy==2.3.2 pandas==2.3.1 scikit-learn==1.5.2 

# PyTorch with CUDA 12.1 support
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu121

pip install transformers==4.53.2 sentence-transformers==3.3.1
pip install jupyter==1.1.1 python-dotenv==1.0.1 pytest==8.3.3

# LangChain Ecosystem (Updated for v0.3.x compatibility)
pip install langchain==0.3.27 langchain-community==0.3.5 langchain-openai==0.3.28

# Other AI Frameworks (Optional - updated versions)
pip install crewai==0.83.5 pyautogen==0.4.3 llama-index==0.12.8

# API Clients and Utilities (Updated August 2025)
pip install openai==1.54.3 anthropic==0.39.0 google-generativeai==0.8.3 huggingface_hub==0.26.2
pip install requests==2.32.3 beautifulsoup4==4.12.3 ollama==0.4.2 chromadb==0.5.20 faiss-cpu==1.9.0

# Security Tools
pip install safety==3.2.11 pip-audit==2.8.0</code></pre>
                        </li>
                        <li><strong>Generate requirements.txt:</strong> After installing, create the file.
                            <pre><code>pip freeze > requirements.txt</code></pre>
                        </li>
                        <li><strong>To Reinstall Later:</strong> Anyone (or any server) can replicate the environment with:
                            <pre><code>pip install -r requirements.txt</code></pre>
                        </li>
                    </ol>
                    
                    <h4>Path B: Advanced Method (`pip-tools`)</h4>
                    <p>This is the recommended best practice for cleaner, more maintainable projects. You define only your direct dependencies, and <code>pip-tools</code> resolves and pins the sub-dependencies.</p>
                    <ol>
                        <li><strong>Install pip-tools</strong>: <code>pip install pip-tools==7.5.0</code></li>
                        <li><strong>Create <code>requirements.in</code></strong>: Manually create this file and list only your top-level dependencies.
                            <pre><code># requirements.in - Updated August 2025
# Core
numpy~=2.3.2
pandas~=2.3.1
python-dotenv~=1.0.1
pytest~=8.3.3

# LangChain Ecosystem (v0.3.x compatible)
langchain~=0.3.27
langchain-openai~=0.3.28

# PyTorch (CUDA 12.1 compatible)
# NOTE: pip-tools does not support --index-url per line.
#       Install torch separately after pip-sync, or use a constraints file.
# torch==2.7.0 --index-url https://download.pytorch.org/whl/cu121

# API and local models
openai~=1.54.3
ollama~=0.4.2

# Vector DB
chromadb~=0.5.20

# Optional: Add other frameworks like crewai or llama-index here
# crewai~=0.83.5</code></pre>
                            <div class="warning">
                                <strong>PyTorch/CUDA Note:</strong>
                                <ul>
                                    <li><code>pip-tools</code> does <b>not</b> support <code>--index-url</code> per line in <code>requirements.in</code>.</li>
                                    <li>After running <code>pip-sync</code> or <code>pip install -r requirements.txt</code>, install PyTorch with CUDA manually:<br>
                                    <code>pip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cu121</code></li>
                                    <li>Alternatively, use a <code>constraints.txt</code> file for advanced workflows. See <a href="https://pip-tools.readthedocs.io/en/latest/#using-pip-compile-with-pip-install">pip-tools docs</a> for details.</li>
                                </ul>
                            </div>
                        </li>
                        <li><strong>Compile <code>requirements.txt</code></strong>: Run the command below. It will generate a <code>requirements.txt</code> file with all dependencies pinned.
                            <pre><code>pip-compile requirements.in</code></pre>
                        </li>
                        <li><strong>Install From Compiled File</strong>: Use the generated file to install everything.
                            <pre><code>pip install -r requirements.txt
# Then install PyTorch with CUDA if needed:
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu121</code></pre>
                        </li>
                    </ol>

                    <div class="warning">
                        <h4>‚ö†Ô∏è Version Conflict Resolution</h4>
                        <p>If you encounter package conflicts:</p>
                        <ol>
                            <li><strong>Create a fresh environment</strong>: <code>python -m venv .tmp_test</code></li>
                            <li><strong>Test individual packages</strong>: Install one problematic package at a time</li>
                            <li><strong>Check compatibility</strong>: Use <code>pip check</code> to identify conflicts</li>
                            <li><strong>Use pip-tools</strong>: Let it resolve complex dependencies automatically</li>
                        </ol>
                    </div>
                </article>
               
                <article class="section" id="section6">
                    <h3>6. GPU Setup for AI Development (Intermediate)</h3>
                    <p>For AI projects involving training or inference with large models, a GPU can significantly speed up computations. This section guides you through setting up NVIDIA GPU support with CUDA.</p>

                    <div class="version-matrix">
                        <h4>GPU Compatibility Matrix (August 2025)</h4>
                        <table>
                            <tr>
                                <th>PyTorch Version</th>
                                <th>CUDA Version</th>
                                <th>Python Support</th>
                                <th>Installation Command</th>
                            </tr>
                            <tr>
                                <td>2.7.0 (Latest)</td>
                                <td>12.1</td>
                                <td>3.11-3.13</td>
                                <td><code>pip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cu121</code></td>
                            </tr>
                            <tr>
                                <td>2.7.0 (CUDA 12.1)</td>
                                <td>12.1</td>
                                <td>3.11-3.13</td>
                                <td><code>pip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cu121</code></td>
                            </tr>
                            <tr>
                                <td>2.7.0 (CPU Only)</td>
                                <td>N/A</td>
                                <td>3.11-3.13</td>
                                <td><code>pip install torch==2.7.0</code></td>
                            </tr>
                        </table>
                    </div>

                    <h4>6.1 Prerequisites</h4>
                    <ul>
                        <li><strong>Hardware</strong>: An NVIDIA GPU (e.g., RTX 3060, 4080, or higher). AMD GPUs are not supported for CUDA-based workflows.</li>
                        <li><strong>OS</strong>: Windows, Linux, or WSL2 on Windows. macOS does not support CUDA.</li>
                        <li><strong>NVIDIA Driver</strong>: Ensure you have the latest NVIDIA drivers installed *before* installing the CUDA toolkit.</li>
                    </ul>

                    <h4>6.2 Install CUDA Toolkit and cuDNN</h4>
                    <ol>
                        <li><strong>Check Required CUDA Version</strong>: PyTorch 2.7.0 supports CUDA 12.1. CUDA 12.1 is recommended for GPU architecture support.</li>
                        <li><strong>Download CUDA Toolkit</strong>: Visit <a href="https://developer.nvidia.com/cuda-toolkit">NVIDIA CUDA Toolkit</a>, select version 12.1, and follow installation prompts.</li>
                        <li><strong>Install cuDNN</strong>: Visit <a href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a>, download the version compatible with your CUDA Toolkit, and follow NVIDIA's instructions to copy the files.</li>
                        <li><strong>Verify Installation</strong>:
                            <pre><code># Check CUDA version
nvcc --version
# Should show CUDA version (e.g., 12.1)</code></pre>
                        </li>
                    </ol>

                    <h4>6.3 Install GPU-Enabled PyTorch</h4>
                    <pre><code># Ensure your virtual environment is active
# For CUDA 12.1 (recommended):
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu121
</code></pre>
                    <p class="note"><strong>pip-tools Note:</strong> If you use <code>pip-tools</code> for dependency management, install PyTorch with CUDA manually after syncing other dependencies, as <code>pip-tools</code> does not support <code>--index-url</code> per line.</p>
                    <p class="note"><strong>Apple Silicon Note</strong>: For macOS with M1/M2/M3, use PyTorch's native Metal Performance Shaders (MPS). The standard <code>pip install torch==2.7.0</code> will include MPS support. Check for availability with <code>torch.backends.mps.is_available()</code>.</p>

                    <h4>6.4 Verify GPU Setup</h4>
                    <p>Create a test script (<code>test_gpu.py</code>):</p>
                    <pre><code>import torch

print(f"‚úì PyTorch version: {torch.__version__}")
print(f"‚úì CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"‚úì GPU device: {torch.cuda.get_device_name(0)}")
    print(f"‚úì CUDA version built with: {torch.version.cuda}")
    print(f"‚úì Number of GPUs: {torch.cuda.device_count()}")
    
    # Test tensor operation
    try:
        x = torch.rand(5, 3).cuda()
        print(f"‚úì Tensor operation successful: {x.mean()}")
    except RuntimeError as e:
        print(f"‚úó CUDA operation failed: {str(e)}")
        print("Common solutions: Reinstall CUDA drivers or use torch==2.7.0+cu121")
# Add a check for Apple Silicon (MPS)
elif torch.backends.mps.is_available():
    print(f"‚úì Apple Silicon MPS available")
    print(f"‚úì Using device: mps")
else:
    print("‚ÑπÔ∏è No GPU detected - using CPU only")</code></pre>
                </article>
               
                <div class="next-steps">
                    <h3>Ready to Build AI Agents?</h3>
                    <p>You've set up your development environment. Now let's create your first AI agent.</p>
                    <a href="#part2" class="cta-button">Continue to AI Development <i class="fas fa-arrow-right"></i></a>
                    <p>Already comfortable with setup? <a href="#part2">Jump to Part 2</a></p>
                </div>
            </section>
           
            <!-- Part 2: AI Development -->
            <section id="part2" class="card">
                <div class="part-header">
                    <h2><i class="fas fa-robot"></i> Part 2: AI Development</h2>
                    <span class="time-estimate">60-90 min</span>
                </div>
               
                <div class="progress-bar">
                    <div class="progress" style="width: 66%"></div>
                </div>
                <div class="timeline">
                    <span>Getting Started</span>
                    <span>Development Ready</span>
                    <span>Production</span>
                </div>
               
                <div class="audience-box">
                    <h3><i class="fas fa-user"></i> Start Here If:</h3>
                    <ul>
                        <li>Your environment is already configured</li>
                        <li>You're ready to build your first AI agent</li>
                        <li>You're working with RAG or agent chains</li>
                    </ul>
                </div>
               
                <p>In this section, you'll create your first AI agent, work with vector databases, and learn debugging techniques. You'll also set up API keys and verify your environment.</p>
               
                <article class="section" id="section7">
                    <h3>7. Project Structure & Best Practices (Beginner)</h3>
                    <h4>7.1 Structure</h4>
                    <pre><code>my-ai-agent/
‚îú‚îÄ‚îÄ .venv/            # Virtual environment
‚îú‚îÄ‚îÄ .vscode/          # VS Code settings
‚îú‚îÄ‚îÄ .env              # Environment variables
‚îú‚îÄ‚îÄ .env.example      # Template for env variables
‚îú‚îÄ‚îÄ .gitignore        # Git ignore file
‚îú‚îÄ‚îÄ requirements.txt  # Python dependencies (generated)
‚îú‚îÄ‚îÄ requirements.in   # Top-level dependencies (if using pip-tools)
‚îú‚îÄ‚îÄ README.md         # Project documentation
‚îú‚îÄ‚îÄ pyproject.toml    # Project configuration (for linters, formatters, etc.)
‚îú‚îÄ‚îÄ src/              # Source code
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ notebooks/        # Jupyter notebooks
‚îî‚îÄ‚îÄ tests/            # Unit tests</code></pre>

                    <h4>7.2 Create .gitignore</h4>
                    <p>A good starting point can be generated from <a href="https://www.toptal.com/developers/gitignore">gitignore.io</a> using terms like: `Python`, `VisualStudioCode`, `venv`.</p>

                    <h4>7.3 Centralizing Tool Configuration (`pyproject.toml`)</h4>
                    <p>
                        For consistent code quality across your project and team members, it's best practice to
                        configure tools like Black (formatter), isort (import sorter), and Ruff (linter/formatter)
                        in <code>pyproject.toml</code> rather than editor-specific settings. This ensures your project's
                        code style is enforced uniformly, regardless of the IDE used.
                    </p>
                    <p>Create or update your <code>pyproject.toml</code> file with the following:</p>
                    <pre><code># pyproject.toml

[tool.black]
line-length = 88
target-version = ['py312'] # Or your Python version

[tool.isort]
profile = "black" # Ensures compatibility with Black's formatting
line_length = 88

[tool.ruff]
line-length = 88
select = ["E", "F", "W", "I", "UP", "C"] # Common linting rules (Error, Flake8, Warning, Isort, PyUpgrade, Complexity)
ignore = ["E501"] # Example: Ignore line length if Black handles it
target-version = "py312" # Or your Python version
</code></pre>
                    <p class="note">
                        <strong>Why this is important:</strong> When these configurations are in <code>pyproject.toml</code>,
                        VS Code (via its Python extension) and other tools like pre-commit hooks will
                        automatically pick them up, making your project's code quality standards portable.
                    </p>
                </article>
               
                <article class="section" id="section8">
                    <h3>8. Debugging Configuration (Intermediate)</h3>
                    <p>Create <code>.vscode/launch.json</code>:</p>
                    <pre><code>{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        },
        {
            "name": "Python: AI Agent (FastAPI)",
            "type": "python",
            "request": "launch",
            "program": "-m",
            "args": ["uvicorn", "src.main:app", "--reload"],
            "console": "integratedTerminal",
            "envFile": "${workspaceFolder}/.env"
        }
    ]
}</code></pre>
                    <p>Create <code>.vscode/settings.json</code>:</p>
                    <pre><code>{
    "python.defaultInterpreterPath": "./.venv/bin/python",
    "python.formatting.provider": "black",
    "editor.formatOnSave": true,
    "python.linting.pylintEnabled": true,
    "python.testing.pytestEnabled": true,
    "python.analysis.typeCheckingMode": "basic",
    "files.autoSave": "afterDelay",
    "editor.codeActionsOnSave": {
        "source.organizeImports": true
    }
}</code></pre>
                    <p class="note">
                        <strong>Note on settings:</strong> While <code>pyproject.toml</code> centrally configures the behavior of
                        tools like Black and Ruff, these <code>.vscode/settings.json</code> lines tell VS Code to
                        actually *use* those tools and enable editor-specific features like "format on save"
                        and test discovery.
                    </p>
                </article>
               
                <article class="section" id="section9">
                    <h3>9. Environment Variables & API Keys (Beginner)</h3>
                    <p><strong>Security</strong>: Never commit API keys to version control. Use environment variables.</p>
                    
                    <div class="critical">
                        <h4>üö® SECURITY WARNING:</h4>
                        <p>Never commit .env files to Git. Add to .gitignore:</p>
                        <pre><code># .gitignore
.env
*.env
env/</code></pre>
                        <p>Use secret management in production (AWS Secrets Manager, HashiCorp Vault)</p>
                    </div>
                    
                    <pre><code># .env file
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here</code></pre>
                    <pre><code># Load variables in Python
import os
from dotenv import load_dotenv

load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")

# Cost estimation function
def estimate_openai_cost(prompt: str, model="gpt-4o-mini") -> float:
    """Estimate API call cost (pricing as of August 2025)"""
    try:
        import tiktoken
    except ImportError:
        print("‚ö†Ô∏è Install tiktoken for cost estimation: pip install tiktoken")
        return 0.0
        
    enc = tiktoken.encoding_for_model(model)
    tokens = len(enc.encode(prompt))
    
    # Pricing examples (check current pricing)
    cost_per_token = {
        "gpt-4o-mini": 0.000005,
        "gpt-4o": 0.000015,
        "claude-3-haiku": 0.000003
    }.get(model, 0.00001)
    
    return tokens * cost_per_token

# Usage example:
# cost = estimate_openai_cost("What is the meaning of life?")
# print(f"Estimated cost: ${cost:.6f}")</code></pre>
                </article>
               
                <article class="section" id="section10">
                    <h3>10. Complete Setup Verification (Beginner)</h3>
                    <p>Create <code>test_setup.py</code> and run it to verify your environment.</p>
                    <pre><code>import sys
import os
from dotenv import load_dotenv

def check_package(package_name):
    try:
        __import__(package_name)
        print(f"‚úì {package_name} is installed.")
    except ImportError:
        print(f"‚úó {package_name} is NOT installed.")

print("üîç Verifying AI Development Setup...")
# Check Python Version
print(f"‚úì Using Python {sys.version}")
# Check Packages
check_package('openai')
check_package('langchain')
check_package('torch')
check_package('dotenv')
# Check API Keys
load_dotenv()
if os.getenv("OPENAI_API_KEY"):
    print("‚úì OPENAI_API_KEY found in .env file.")
else:
    print("‚úó OPENAI_API_KEY is missing from .env file.")
print("\nüéâ Verification complete.")</code></pre>
                </article>
               
                <article class="section" id="section11">
                    <h3>11. Your First AI Agent: "Hello, AI World" (Beginner)</h3>
                    
                    <div class="critical">
                        <h4>üö® UPDATED FOR LANGCHAIN 0.3.x</h4>
                        <p>Critical syntax updates for compatibility with current versions:</p>
                    </div>
                    
                    <h4>11.1 Using an API (OpenAI) - Updated Syntax</h4>
                    <p>Create <code>src/hello_agent.py</code>:</p>
                    <pre><code>import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# Updated for LangChain 0.3.x compatibility
# Ensure you have installed: pip install langchain-openai==0.3.28

def run_agent():
    load_dotenv()
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå Error: OPENAI_API_KEY not found in .env file.")
        return

    # Updated import and initialization for v0.3.x
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
    question = "What is the main benefit of using a virtual environment in Python?"
    print(f"ü§ñ Asking AI: '{question}'")
    
    try:
        # Updated message format for v0.3.x
        messages = [HumanMessage(content=question)]
        response = llm.invoke(messages)
        print(f"‚úÖ AI Response: {response.content}")
    except Exception as e:
        print(f"‚ùå An error occurred: {e}")

if __name__ == "__main__":
    run_agent()</code></pre>

                    <h4>11.2 Using Direct OpenAI API (Alternative)</h4>
                    <p>Create <code>src/hello_agent_direct.py</code>:</p>
                    <pre><code>import os
from dotenv import load_dotenv
from openai import OpenAI

# This code uses the modern OpenAI v1.x+ library
# Ensure you have installed: pip install openai>=1.54.0

def run_direct_agent():
    load_dotenv()
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå Error: OPENAI_API_KEY not found in .env file.")
        return

    client = OpenAI()  # API key is read from OPENAI_API_KEY env var by default
    question = "What is the main benefit of using a virtual environment in Python?"
    print(f"ü§ñ Asking AI: '{question}'")
    
    try:
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant providing concise answers."},
                {"role": "user", "content": question}
            ]
        )
        response = completion.choices[0].message.content
        print(f"‚úÖ AI Response: {response}")
    except Exception as e:
        print(f"‚ùå An error occurred: {e}")

if __name__ == "__main__":
    run_direct_agent()</code></pre>

                    <h4>11.3 Using a Local Model (Ollama)</h4>
                    <ol>
                        <li>Install Ollama from <a href="https://ollama.com">ollama.com</a>.</li>
                        <li>Pull a model: <code>ollama pull llama3.1</code> (or a smaller model like <code>phi3</code>).</li>
                        <li>Create <code>src/hello_agent_local.py</code>:</li>
                    </ol>
                    <pre><code>import ollama

def run_local_agent():
    question = "What is the main benefit of using a virtual environment in Python?"
    model_name = "llama3.1"
    print(f"ü§ñ Asking local AI ({model_name}): '{question}'")

    try:
        response = ollama.chat(
            model=model_name,
            messages=[{'role': 'user', 'content': question}]
        )
        print(f"‚úÖ AI Response: {response['message']['content']}")
    except Exception as e:
        print(f"‚ùå An error occurred. Is the Ollama app running? {e}")

if __name__ == "__main__":
    run_local_agent()</code></pre>
                </article>
               
                <article class="section" id="section15">
                    <h3>15. Advanced Vector Database Setup (Intermediate)</h3>
                    <p>Use ChromaDB for local RAG development.</p>
                    <pre><code># Updated ChromaDB example for v0.5.x
import chromadb

# Create a persistent client that saves to disk
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(name="my_documents")

# Add documents with IDs and metadata
collection.add(
    documents=["This is a document about Python.", "This is a document about AI."],
    ids=["doc1", "doc2"],
    metadatas=[{"topic": "python"}, {"topic": "ai"}]
)

# Query the collection
results = collection.query(query_texts=["What is AI?"], n_results=1)
print("Query results:", results['documents'][0])</code></pre>

                    <h4>15.1 Advanced RAG with LangChain</h4>
                    <pre><code># Example: RAG chain with updated LangChain 0.3.x syntax
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Create sample documents
docs = [
    Document(page_content="Python is a programming language.", metadata={"source": "doc1"}),
    Document(page_content="AI is artificial intelligence.", metadata={"source": "doc2"})
]

# Create vector store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(docs, embeddings)
retriever = vectorstore.as_retriever()

# Create RAG chain
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
llm = ChatOpenAI(model="gpt-4o-mini")

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Use the chain
result = rag_chain.invoke("What is Python?")
print(result)</code></pre>
                </article>
               
                <div class="next-steps">
                    <h3>Ready for Production?</h3>
                    <p>You've built functional AI agents. Now let's prepare them for deployment.</p>
                    <a href="#part3" class="cta-button">Continue to Production <i class="fas fa-arrow-right"></i></a>
                    <p>Just need development setup? <a href="#quick-reference">See Cheat Sheet</a></p>
                </div>
            </section>
           
            <!-- Part 3: Production Deployment -->
            <section id="part3" class="card">
                <div class="part-header">
                    <h2><i class="fas fa-server"></i> Part 3: Production Deployment</h2>
                    <span class="time-estimate">90-120 min</span>
                </div>
               
                <div class="progress-bar">
                    <div class="progress" style="width: 100%"></div>
                </div>
                <div class="timeline">
                    <span>Getting Started</span>
                    <span>Development Ready</span>
                    <span>Production</span>
                </div>
               
                <div class="audience-box">
                    <h3><i class="fas fa-user"></i> Start Here If:</h3>
                    <ul>
                        <li>Your agents are working locally</li>
                        <li>You're ready to deploy to production</li>
                        <li>You need CI/CD and monitoring</li>
                    </ul>
                </div>
               
                <p>This section covers containerization, API serving, experiment tracking, and automation. You'll also build a complete RAG agent project.</p>
               
                <div class="roadmap">
                    <h3>Deployment Roadmap</h3>
                    <ol>
                    <li>Containerize application (Docker)</li>
                    <li>Implement API serving (FastAPI)</li>
                    <li>Add monitoring (MLflow)</li>
                    <li>Automate testing (pre-commit)</li>
                    </ol>
                </div>
               
                <article class="section" id="section12">
                    <h3>12. Docker & Containerization (Intermediate)</h3>
                    <p>Create a <code>Dockerfile</code> to package your application.</p>
                    <pre><code># Use Python 3.12 slim image
FROM python:3.12-slim

WORKDIR /app

# For security, create a non-root user
RUN addgroup --system appuser && adduser --system --ingroup appuser appuser

# Copy requirements first for better caching
COPY --chown=appuser:appuser requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY --chown=appuser:appuser . .

# Add health check
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8000/health || exit 1

# Switch to the non-privileged user
USER appuser

# Expose port and define entry point for FastAPI
EXPOSE 8000
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]</code></pre>
                    <p>Build and run the container:</p>
                    <pre><code>docker build -t my-ai-agent .
docker run -p 8000:8000 --env-file .env my-ai-agent</code></pre>
                </article>
               
                <article class="section" id="section13">
                    <h3>13. Experiment Tracking & MLOps (Intermediate)</h3>
                    <p>Use MLflow to track experiments.</p>
                    <pre><code>pip install mlflow==2.19.0
mlflow ui # Access at http://localhost:5000</code></pre>
                    <pre><code># Example: track_experiment.py
import mlflow
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv

load_dotenv()
mlflow.set_experiment("Agent Prompts")

with mlflow.start_run():
    mlflow.log_param("model", "gpt-4o-mini")
    mlflow.log_param("temperature", 0.5)

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
    messages = [HumanMessage(content="Test prompt")]
    response = llm.invoke(messages)
    
    # Log response details if available
    mlflow.log_text(response.content, "response.txt")
    if hasattr(response, 'usage_metadata'):
        mlflow.log_metric("tokens_used", response.usage_metadata.get('total_tokens', 0))</code></pre>
                </article>
               
                <article class="section" id="section14">
                    <h3>14. FastAPI for Model Serving (Intermediate)</h3>
                    <p>Create <code>src/main.py</code> to build a production-ready API.</p>
                    <pre><code>import os
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Load environment variables first
load_dotenv()

# Initialize rate limiter
limiter = Limiter(key_func=get_remote_address)
app = FastAPI(title="AI Agent Server", version="1.0.0")
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# --- State object to hold the model ---
class AppState:
    llm = None

# --- Pydantic model for request body ---
class PredictionRequest(BaseModel):
    prompt: str
    model: str = "gpt-4o-mini"
    temperature: float = 0.5

# --- App startup event to initialize the model ---
@app.on_event("startup")
async def startup_event():
    # Initialize with error handling
    if not os.getenv("OPENAI_API_KEY"):
        print("üî¥ CRITICAL: OPENAI_API_KEY is not set. The /predict endpoint will not work.")
        AppState.llm = None
    else:
        try:
            AppState.llm = ChatOpenAI(model="gpt-4o-mini")
            print("‚úÖ OpenAI client initialized successfully.")
        except Exception as e:
            print(f"üî¥ CRITICAL: Could not initialize OpenAI client: {e}")
            AppState.llm = None

# --- API Endpoints ---
@app.post("/predict")
@limiter.limit("5/minute")  # Limit to 5 requests per minute per IP
async def predict(request: PredictionRequest, req: Request):
    if AppState.llm is None:
        raise HTTPException(
            status_code=503, 
            detail="AI model is not available. Check server logs for initialization errors."
        )
    
    try:
        messages = [HumanMessage(content=request.prompt)]
        # Use the model instance from the app state
        response = AppState.llm.invoke(messages)
        return {"response": response.content}
    except Exception as e:
        # Catch potential API errors during invocation
        raise HTTPException(status_code=500, detail=f"An error occurred while processing the request: {e}")

@app.get("/health")
async def health_check():
    health_status = {
        "status": "healthy",
        "message": "AI Agent Server is running",
        "model_initialized": AppState.llm is not None
    }
    return health_status</code></pre>
                    <p>Run the development server:</p>
                    <pre><code>pip install fastapi==0.115.6 uvicorn==0.32.1 pydantic==2.10.1 slowapi==0.1.9
uvicorn src.main:app --reload --port 8000</code></pre>
                    <p class="note">Navigate to <code>http://localhost:8000/docs</code> to see interactive API documentation.</p>
                    <p class="note"><strong>Production Note:</strong> For production, consider adding request timeouts and authentication to your FastAPI endpoints. See <a href="https://fastapi.tiangolo.com/advanced/security/">FastAPI Security</a> for best practices.</p>
                </article>
               
                <article class="section" id="section16">
                    <h3>16. Code Quality Automation (Intermediate)</h3>
                    <p>Use <code>pre-commit</code> to run checks before every commit.</p>
                    <pre><code>pip install pre-commit==4.0.1
pre-commit install</code></pre>
                    <p>Create <code>.pre-commit-config.yaml</code>:</p>
                    <pre><code>repos:
  - repo: https://github.com/psf/black-pre-commit-mirror
    rev: 24.10.0
    hooks:
      - id: black
  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.8.4
    hooks:
      - id: ruff
        args: [--fix]</code></pre>
                </article>
               
                <article class="section" id="section20">
                    <h3>20. Integrated Mini-Project: RAG Agent with a FastAPI Endpoint (Advanced)</h3>
                    <p>This final example ties together several concepts from this guide into a single, functional application. We will build a simple Retrieval-Augmented Generation (RAG) API using <strong>FastAPI</strong>, <strong>LangChain</strong>, and <strong>ChromaDB</strong>.</p>
                    <p><strong>What this project demonstrates:</strong></p>
                    <ul>
                        <li><strong>Project Structure:</strong> Using the <code>src/</code> directory for modular code</li>
                        <li><strong>Dependency Management:</strong> Using packages like <code>fastapi</code>, <code>langchain</code>, and <code>chromadb</code></li>
                        <li><strong>Vector Databases:</strong> Setting up a persistent ChromaDB store</li>
                        <li><strong>Advanced Chains:</strong> Building a RAG chain with modern LangChain (LCEL)</li>
                        <li><strong>API Serving:</strong> Exposing the RAG chain through a secure FastAPI endpoint</li>
                        <li><strong>Environment Variables:</strong> Loading API keys correctly with <code>python-dotenv</code></li>
                    </ul>

                    <div class="quick-start">
                        <h4>üöÄ Project Goal</h4>
                        <p>To create an API endpoint <code>/query</code> that accepts a question, searches a small knowledge base for relevant context, and uses an LLM to generate an answer based on that context.</p>
                    </div>

                    <h4>Step 1: Update Project Structure and Dependencies</h4>
                    <p>First, ensure your project has the following structure and that the necessary packages are installed. We will create three new files: <code>src/vector_store.py</code>, <code>src/rag_chain.py</code>, and <code>src/main_rag_api.py</code>.</p>
                    <pre><code>my-ai-agent/
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ chroma_db/        # Will be created automatically by ChromaDB
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py # New: Logic for setting up ChromaDB
‚îÇ   ‚îú‚îÄ‚îÄ rag_chain.py    # New: Logic for the RAG chain
‚îÇ   ‚îî‚îÄ‚îÄ main_rag_api.py # New: The FastAPI application
‚îî‚îÄ‚îÄ ...</code></pre>

                    <p>Ensure you have the required packages installed:</p>
                    <p>See <a href="https://pypi.org/project/fastapi/">fastapi</a>, <a href="https://pypi.org/project/uvicorn/">uvicorn</a>, <a href="https://pypi.org/project/langchain/">langchain</a>, <a href="https://pypi.org/project/langchain-openai/">langchain-openai</a>, <a href="https://pypi.org/project/langchain-community/">langchain-community</a>, <a href="https://pypi.org/project/openai/">openai</a>, <a href="https://pypi.org/project/chromadb/">chromadb</a>, <a href="https://pypi.org/project/python-dotenv/">python-dotenv</a>, <a href="https://pypi.org/project/sentence-transformers/">sentence-transformers</a> on PyPI.</p>
                    <pre><code>pip install fastapi uvicorn "langchain[llms]" langchain-openai langchain-community openai chromadb python-dotenv sentence-transformers</code></pre>
                    <p class="note">The <code>sentence-transformers</code> package is used by ChromaDB's default embedding function if you don't provide one, but we will explicitly use OpenAI's embeddings for better performance.</p>

                    <h4>Step 2: Create the Vector Store (<code>src/vector_store.py</code>)</h4>
                    <p>This module will handle setting up our document store. It will initialize ChromaDB, add documents to it, and create a retriever object that LangChain can use.</p>
                    <pre><code># src/vector_store.py
import chromadb
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# Define the persistent directory
CHROMA_PATH = "./chroma_db"

def get_retriever():
    """
    Initializes and returns a ChromaDB retriever from a predefined set of documents.
    """
    # Sample documents for our knowledge base
    docs = [
        Document(
            page_content="VS Code is a lightweight but powerful source code editor from Microsoft.",
            metadata={"source": "doc1", "topic": "tools"}
        ),
        Document(
            page_content="A virtual environment is a self-contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages.",
            metadata={"source": "doc2", "topic": "python"}
        ),
        Document(
            page_content="RAG, or Retrieval-Augmented Generation, is a technique for enhancing the accuracy and reliability of large language models (LLMs) with facts fetched from external sources.",
            metadata={"source": "doc3", "topic": "ai"}
        ),
        Document(
            page_content="FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.8+ based on standard Python type hints.",
            metadata={"source": "doc4", "topic": "tools"}
        ),
    ]

    # Initialize OpenAI embeddings
    embeddings = OpenAIEmbeddings()

    # Create a new ChromaDB persistent client
    # This will save the vector store to disk in the 'chroma_db' directory
    db_client = chromadb.PersistentClient(path=CHROMA_PATH)

    # Create or load the vector store
    vectorstore = Chroma.from_documents(
        documents=docs, 
        embedding=embeddings,
        persist_directory=CHROMA_PATH
    )

    # Create and return a retriever
    # 'k=2' means it will retrieve the top 2 most relevant documents
    return vectorstore.as_retriever(search_kwargs={"k": 2})

if __name__ == '__main__':
    # A simple test to verify the retriever is working
    print("Initializing and testing the vector store...")
    retriever = get_retriever()
    test_query = "What is RAG?"
    results = retriever.invoke(test_query)
    print(f"Retrieved {len(results)} documents for query: '{test_query}'")
    for doc in results:
        print(f"- {doc.page_content}")
    print("\nVector store setup complete and verified.")
</code></pre>

                    <h4>Step 3: Create the RAG Chain (<code>src/rag_chain.py</code>)</h4>
                    <p>This module defines the core logic of our AI. It imports the retriever from the previous step and chains it together with a prompt template and an LLM to create the final RAG chain.</p>
                    <pre><code># src/rag_chain.py
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from src.vector_store import get_retriever

def get_rag_chain():
    """
    Creates and returns a RAG chain using the vector store retriever.
    """
    retriever = get_retriever()
    
    # RAG prompt template
    template = """You are an assistant for question-answering tasks. 
    Use the following pieces of retrieved context to answer the question. 
    If you don't know the answer, just say that you don't know. 
    Keep the answer concise.

    Context: {context} 

    Question: {question} 

    Answer:"""
    
    prompt = ChatPromptTemplate.from_template(template)
    
    # Initialize the LLM
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    # Create the RAG chain using LangChain Expression Language (LCEL)
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

if __name__ == '__main__':
    # A simple test to verify the chain is working
    print("Testing the RAG chain...")
    chain = get_rag_chain()
    response = chain.invoke("What is FastAPI?")
    print(response)
</code></pre>

                    <h4>Step 4: Build the FastAPI App (<code>src/main_rag_api.py</code>)</h4>
                    <p>This is the entry point for our API. It loads the RAG chain, defines the request and response models, and creates an endpoint to handle user queries.</p>
                    <pre><code># src/main_rag_api.py
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from dotenv import load_dotenv
from src.rag_chain import get_rag_chain

# Load environment variables from .env file
load_dotenv()

# Initialize the FastAPI app
app = FastAPI(
    title="RAG API Server",
    version="1.0",
    description="A simple API server for a Retrieval-Augmented Generation agent.",
)

# --- Pydantic Models for Request and Response ---
class QueryRequest(BaseModel):
    question: str

class QueryResponse(BaseModel):
    answer: str

# --- API Endpoints ---
@app.get("/", summary="Health Check")
async def health_check():
    """A simple health check endpoint to confirm the server is running."""
    return {"status": "ok", "message": "RAG API is running"}

@app.post("/query", response_model=QueryResponse, summary="Query the RAG Agent")
async def query_agent(request: QueryRequest):
    """
    Receives a question, processes it through the RAG chain, and returns the answer.
    """
    if not os.getenv("OPENAI_API_KEY"):
        raise HTTPException(status_code=500, detail="OPENAI_API_KEY not found in environment variables.")
    
    if not request.question:
        raise HTTPException(status_code=400, detail="Question field cannot be empty.")
    
    try:
        # Get the singleton RAG chain instance
        rag_chain = get_rag_chain()
        answer = rag_chain.invoke(request.question)
        return QueryResponse(answer=answer)
    except Exception as e:
        # A generic error handler for issues during chain invocation
        raise HTTPException(status_code=500, detail=f"An error occurred: {e}")

# To run this app:
# uvicorn src.main_rag_api:app --reload --port 8000
</code></pre>

                    <h4>Step 5: Run and Test Your Integrated Application</h4>
                    <p>With all the files in place, you can now run your API server.</p>

                    <ol>
                        <li><strong>Ensure your <code>.env</code> file contains your <code>OPENAI_API_KEY</code>.</strong></li>
                        <li>
                            <strong>Start the Server:</strong> Open your terminal (with the virtual environment activated) and run:
                            <pre><code>uvicorn src.main_rag_api:app --reload --port 8000</code></pre>
                        </li>
                        <li>
                            <strong>Test via Interactive Docs:</strong> Open your browser and navigate to <a href="http://127.0.0.1:8000/docs">http://127.0.0.1:8000/docs</a>. You will see the FastAPI interface.
                            <ul>
                                <li>Expand the <code>/query</code> endpoint.</li>
                                <li>Click "Try it out".</li>
                                <li>Enter a question in the request body, such as: <code>"What is the purpose of a virtual environment?"</code></li>
                                <li>Click "Execute". You should see the AI-generated response based on the context from your vector store.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Test via <code>curl</code> (Optional):</strong>
                            <pre><code>curl -X POST "http://127.0.0.1:8000/query" \
-H "Content-Type: application/json" \
-d '{"question": "What is VS Code?"}'</code></pre>
                            <p><strong>Expected output:</strong></p>
                            <pre><code>{"answer":"VS Code is a lightweight but powerful source code editor from Microsoft."}</code></pre>
                        </li>
                    </ol>
                    <p class="note"><strong>Production Tip:</strong> For public deployments, add authentication and rate limiting to your FastAPI endpoints. See <a href="https://fastapi.tiangolo.com/advanced/security/">FastAPI Security</a> for best practices.</p>
                </article>
            </section>
           
            <!-- Shared Resources -->
            <section id="shared-resources" class="card">
                <h2><i class="fas fa-toolbox"></i> Shared Resources</h2>
               
                <article class="section" id="section18">
                    <h3>18. Troubleshooting & Version Conflicts</h3>
                    
                    <div class="critical">
                        <h4>üö® Common Version Conflict Solutions</h4>
                    </div>
                    
                    <h4>18.1 Package Installation Errors</h4>
                    <ul>
                        <li><strong>Dependency Conflicts</strong>: Create a completely new, empty virtual environment (<code>python -m venv .tmp_venv</code>), activate it, and try to <code>pip install</code> only the problematic package. This will help isolate the issue.</li>
                        <li><strong>LangChain Import Errors</strong>: If you see <code>ModuleNotFoundError: No module named 'langchain_openai'</code>, ensure you've installed the separate package: <code>pip install langchain-openai==0.3.28</code></li>
                        <li><strong>PyTorch CUDA Issues</strong>: Verify your CUDA installation with <code>nvcc --version</code> and ensure it matches the PyTorch version you're installing.</li>
                    </ul>

                    <h4>18.2 LangChain Migration Issues</h4>
                    <p>If upgrading from older LangChain versions:</p>
                    <pre><code># Use the migration CLI tool
pip install langchain-cli
langchain-cli migrate --diff [path to code]  # Preview changes
langchain-cli migrate [path to code]         # Apply changes</code></pre>
                    
                    <h4>18.3 API Key and Authentication Errors</h4>
                    <ul>
                        <li><strong>Environment Variables</strong>: Ensure there are no typos in your <code>.env</code> file. Variable names are case-sensitive.</li>
                        <li><strong>API Key Validation</strong>: Verify your billing status and rate limits on the provider's dashboard (e.g., <a href="https://platform.openai.com/usage">OpenAI Usage Dashboard</a>).</li>
                        <li><strong>Permissions</strong>: Check that your API key has the necessary permissions for the models you're trying to use.</li>
                    </ul>

                    <h4>18.4 GPU and CUDA Issues</h4>
                    <ul>
                        <li><strong>Driver Issues</strong>: Run <code>nvidia-smi</code> to ensure your driver sees the GPU.</li>
                        <li><strong>CUDA Version Mismatch</strong>: Double-check that your installed CUDA version matches the version PyTorch was built with by running the <code>test_gpu.py</code> script.</li>
                        <li><strong>Memory Issues</strong>: Use <code>torch.cuda.empty_cache()</code> to clear GPU memory if you encounter out-of-memory errors.</li>
                    </ul>

                    <h4>18.5 Version Checking Commands</h4>
                    <p>Use these commands to verify your current package versions:</p>
                    <pre><code># Check specific package versions
pip show torch langchain langchain-openai openai

# Check for package conflicts
pip check

# List all installed packages
pip list

# Check Python version and location
which python</code></pre>
                </article>
               
                <article class="section" id="section19">
                    <h3>19. Changelog</h3>
                    <ul>
                        <li><strong>August 5, 2025 (MAJOR UPDATE - v3.0)</strong>:
                            <ul>
                                <li><strong>CRITICAL FIXES APPLIED</strong>:
                                    <ul>
                                        <li>Updated PyTorch to 2.7.0 with CUDA 12.1 support</li>
                                        <li>Fixed LangChain integration for 0.3.x compatibility with new import syntax</li>
                                        <li>Updated all package versions to August 2025 standards</li>
                                        <li>Resolved OpenAI API compatibility issues</li>
                                        <li>Added Python 3.13 compatibility verification</li>
                                    </ul>
                                </li>
                                <li><strong>NEW FEATURES</strong>:
                                    <ul>
                                        <li>Comprehensive version conflict resolution strategies</li>
                                        <li>Updated GPU compatibility matrix with architecture support</li>
                                        <li>Enhanced troubleshooting section with specific error solutions</li>
                                        <li>Added package evolution warnings and version checking commands</li>
                                        <li>Improved FastAPI examples with proper error handling</li>
                                    </ul>
                                </li>
                                <li><strong>PACKAGE VERSION UPDATES</strong>:
                                    <ul>
                                        <li>PyTorch: 2.3.1 ‚Üí 2.7.0 (CUDA 12.1)</li>
                                        <li>LangChain-OpenAI: 0.1.7 ‚Üí 0.3.28 (breaking changes)</li>
                                        <li>NumPy: 1.26.4 ‚Üí 2.3.2</li>
                                        <li>Pandas: 2.2.2 ‚Üí 2.3.1</li>
                                        <li>Transformers: 4.41.2 ‚Üí 4.53.2</li>
                                        <li>OpenAI: 1.35.7 ‚Üí 1.54.3</li>
                                        <li>All other packages updated to latest stable versions</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>
                </article>
               
                <div class="section" id="quick-reference">
                    <h3>Quick Reference</h3>
                    <div class="version-matrix">
                        <h4>Essential Commands</h4>
                        <table>
                            <tr>
                                <th>Task</th>
                                <th>Command</th>
                            </tr>
                            <tr>
                                <td>Create virtual environment</td>
                                <td><code>python -m venv .venv</code></td>
                            </tr>
                            <tr>
                                <td>Activate environment (Windows)</td>
                                <td><code>.venv\Scripts\activate</code></td>
                            </tr>
                            <tr>
                                <td>Activate environment (Mac/Linux)</td>
                                <td><code>source .venv/bin/activate</code></td>
                            </tr>
                            <tr>
                                <td>Install core packages</td>
                                <td><code>pip install langchain-openai chromadb</code></td>
                            </tr>
                            <tr>
                                <td>Run FastAPI</td>
                                <td><code>uvicorn src.main:app --reload</code></td>
                            </tr>
                            <tr>
                                <td>Build Docker image</td>
                                <td><code>docker build -t my-ai-agent .</code></td>
                            </tr>
                        </table>
                    </div>
                   
                    <div class="framework-comparison">
                        <h4>Common Package Versions</h4>
                        <table>
                            <tr>
                                <th>Package</th>
                                <th>Version</th>
                                <th>Type</th>
                            </tr>
                            <tr>
                                <td>Python</td>
                                <td>3.12.7</td>
                                <td>Core</td>
                            </tr>
                            <tr>
                                <td>PyTorch</td>
                                <td>2.7.0</td>
                                <td>ML</td>
                            </tr>
                            <tr>
                                <td>LangChain</td>
                                <td>0.3.27</td>
                                <td>AI</td>
                            </tr>
                            <tr>
                                <td>FastAPI</td>
                                <td>0.115.6</td>
                                <td>Web</td>
                            </tr>
                            <tr>
                                <td>ChromaDB</td>
                                <td>0.5.20</td>
                                <td>Vector DB</td>
                            </tr>
                        </table>
                    </div>
                </div>
            </section>
            <footer class="card">
                <h2>Contact & Feedback</h2>
                <p>Author: <strong>Stephen Hope</strong><br>
                Email: <a href="mailto:sbhope@gmail.com">sbhope@gmail.com</a><br>
                Last Updated: August 5, 2025</p>
                <p>Found an issue or have suggestions? <a href="mailto:sbhope@gmail.com">Contact the author</a>.</p>
            </footer>
        </main>
    </div>

    <script>
        // Copy button functionality
        document.querySelectorAll('.copy-button').forEach(button => {
            button.addEventListener('click', function() {
                const codeBlock = this.parentElement.querySelector('code');
                const textArea = document.createElement('textarea');
                textArea.value = codeBlock.textContent;
                document.body.appendChild(textArea);
                textArea.select();
                document.execCommand('copy');
                document.body.removeChild(textArea);

                const originalText = this.textContent;
                this.textContent = 'Copied!';
                this.classList.add('copied');

                setTimeout(() => {
                    this.textContent = originalText;
                    this.classList.remove('copied');
                }, 2000);
            });
        });

        // Download button functionality
        document.getElementById('download-pdf').addEventListener('click', function() {
            this.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Preparing Download...';
            setTimeout(() => {
                this.innerHTML = '<i class="fas fa-check"></i> Download Complete!';
                setTimeout(() => {
                    this.innerHTML = '<i class="fas fa-file-pdf"></i> Download PDF';
                }, 2000);
            }, 1500);
        });

        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    window.scrollTo({
                        top: target.offsetTop - 20,
                        behavior: 'smooth'
                    });
                }
            });
        });
    </script>
</body>
</html>
